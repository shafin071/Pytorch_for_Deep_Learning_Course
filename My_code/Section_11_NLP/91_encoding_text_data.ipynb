{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Text Generation\n",
    "\n",
    "## Generating Text (encoded variables)\n",
    "\n",
    "We saw how to generate continuous values, now let's see how to generalize this to generate categorical sequences (such as words or letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5447699"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Entire Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique chars\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'T')\n",
      "(1, 'x')\n",
      "(2, 'V')\n",
      "(3, 's')\n",
      "(4, 'c')\n",
      "(5, '}')\n",
      "(6, 'J')\n",
      "(7, 'A')\n",
      "(8, '0')\n",
      "(9, '7')\n",
      "(10, '>')\n",
      "(11, 'S')\n",
      "(12, 'D')\n",
      "(13, 't')\n",
      "(14, '4')\n",
      "(15, '(')\n",
      "(16, '|')\n",
      "(17, 'I')\n",
      "(18, 'F')\n",
      "(19, ';')\n",
      "(20, '!')\n",
      "(21, '[')\n",
      "(22, 'i')\n",
      "(23, 'Z')\n",
      "(24, 'M')\n",
      "(25, 'j')\n",
      "(26, 'Y')\n",
      "(27, '6')\n",
      "(28, 'w')\n",
      "(29, 'o')\n",
      "(30, 'y')\n",
      "(31, '&')\n",
      "(32, 'n')\n",
      "(33, ' ')\n",
      "(34, '<')\n",
      "(35, '3')\n",
      "(36, '.')\n",
      "(37, 'd')\n",
      "(38, '\\n')\n",
      "(39, ',')\n",
      "(40, 'g')\n",
      "(41, 'k')\n",
      "(42, '\"')\n",
      "(43, 'O')\n",
      "(44, ':')\n",
      "(45, 'm')\n",
      "(46, '_')\n",
      "(47, 'e')\n",
      "(48, 'Q')\n",
      "(49, 'W')\n",
      "(50, 'K')\n",
      "(51, 'H')\n",
      "(52, '8')\n",
      "(53, 'v')\n",
      "(54, 'p')\n",
      "(55, 'C')\n",
      "(56, 'r')\n",
      "(57, '?')\n",
      "(58, 'q')\n",
      "(59, 'h')\n",
      "(60, \"'\")\n",
      "(61, 'N')\n",
      "(62, 'X')\n",
      "(63, '9')\n",
      "(64, '5')\n",
      "(65, 'a')\n",
      "(66, 'P')\n",
      "(67, 'l')\n",
      "(68, 'E')\n",
      "(69, ']')\n",
      "(70, 'G')\n",
      "(71, 'u')\n",
      "(72, 'z')\n",
      "(73, '`')\n",
      "(74, '-')\n",
      "(75, '2')\n",
      "(76, 'U')\n",
      "(77, 'B')\n",
      "(78, 'f')\n",
      "(79, 'R')\n",
      "(80, '1')\n",
      "(81, ')')\n",
      "(82, 'b')\n",
      "(83, 'L')\n"
     ]
    }
   ],
   "source": [
    "# There are 84 pairs\n",
    "for pair in enumerate(all_characters):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'T',\n",
       " 1: 'x',\n",
       " 2: 'V',\n",
       " 3: 's',\n",
       " 4: 'c',\n",
       " 5: '}',\n",
       " 6: 'J',\n",
       " 7: 'A',\n",
       " 8: '0',\n",
       " 9: '7',\n",
       " 10: '>',\n",
       " 11: 'S',\n",
       " 12: 'D',\n",
       " 13: 't',\n",
       " 14: '4',\n",
       " 15: '(',\n",
       " 16: '|',\n",
       " 17: 'I',\n",
       " 18: 'F',\n",
       " 19: ';',\n",
       " 20: '!',\n",
       " 21: '[',\n",
       " 22: 'i',\n",
       " 23: 'Z',\n",
       " 24: 'M',\n",
       " 25: 'j',\n",
       " 26: 'Y',\n",
       " 27: '6',\n",
       " 28: 'w',\n",
       " 29: 'o',\n",
       " 30: 'y',\n",
       " 31: '&',\n",
       " 32: 'n',\n",
       " 33: ' ',\n",
       " 34: '<',\n",
       " 35: '3',\n",
       " 36: '.',\n",
       " 37: 'd',\n",
       " 38: '\\n',\n",
       " 39: ',',\n",
       " 40: 'g',\n",
       " 41: 'k',\n",
       " 42: '\"',\n",
       " 43: 'O',\n",
       " 44: ':',\n",
       " 45: 'm',\n",
       " 46: '_',\n",
       " 47: 'e',\n",
       " 48: 'Q',\n",
       " 49: 'W',\n",
       " 50: 'K',\n",
       " 51: 'H',\n",
       " 52: '8',\n",
       " 53: 'v',\n",
       " 54: 'p',\n",
       " 55: 'C',\n",
       " 56: 'r',\n",
       " 57: '?',\n",
       " 58: 'q',\n",
       " 59: 'h',\n",
       " 60: \"'\",\n",
       " 61: 'N',\n",
       " 62: 'X',\n",
       " 63: '9',\n",
       " 64: '5',\n",
       " 65: 'a',\n",
       " 66: 'P',\n",
       " 67: 'l',\n",
       " 68: 'E',\n",
       " 69: ']',\n",
       " 70: 'G',\n",
       " 71: 'u',\n",
       " 72: 'z',\n",
       " 73: '`',\n",
       " 74: '-',\n",
       " 75: '2',\n",
       " 76: 'U',\n",
       " 77: 'B',\n",
       " 78: 'f',\n",
       " 79: 'R',\n",
       " 80: '1',\n",
       " 81: ')',\n",
       " 82: 'b',\n",
       " 83: 'L'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder: num --> letter\n",
    "\n",
    "decoder = dict(enumerate(all_characters))\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder: num --> letter\n",
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 0,\n",
       " 'x': 1,\n",
       " 'V': 2,\n",
       " 's': 3,\n",
       " 'c': 4,\n",
       " '}': 5,\n",
       " 'J': 6,\n",
       " 'A': 7,\n",
       " '0': 8,\n",
       " '7': 9,\n",
       " '>': 10,\n",
       " 'S': 11,\n",
       " 'D': 12,\n",
       " 't': 13,\n",
       " '4': 14,\n",
       " '(': 15,\n",
       " '|': 16,\n",
       " 'I': 17,\n",
       " 'F': 18,\n",
       " ';': 19,\n",
       " '!': 20,\n",
       " '[': 21,\n",
       " 'i': 22,\n",
       " 'Z': 23,\n",
       " 'M': 24,\n",
       " 'j': 25,\n",
       " 'Y': 26,\n",
       " '6': 27,\n",
       " 'w': 28,\n",
       " 'o': 29,\n",
       " 'y': 30,\n",
       " '&': 31,\n",
       " 'n': 32,\n",
       " ' ': 33,\n",
       " '<': 34,\n",
       " '3': 35,\n",
       " '.': 36,\n",
       " 'd': 37,\n",
       " '\\n': 38,\n",
       " ',': 39,\n",
       " 'g': 40,\n",
       " 'k': 41,\n",
       " '\"': 42,\n",
       " 'O': 43,\n",
       " ':': 44,\n",
       " 'm': 45,\n",
       " '_': 46,\n",
       " 'e': 47,\n",
       " 'Q': 48,\n",
       " 'W': 49,\n",
       " 'K': 50,\n",
       " 'H': 51,\n",
       " '8': 52,\n",
       " 'v': 53,\n",
       " 'p': 54,\n",
       " 'C': 55,\n",
       " 'r': 56,\n",
       " '?': 57,\n",
       " 'q': 58,\n",
       " 'h': 59,\n",
       " \"'\": 60,\n",
       " 'N': 61,\n",
       " 'X': 62,\n",
       " '9': 63,\n",
       " '5': 64,\n",
       " 'a': 65,\n",
       " 'P': 66,\n",
       " 'l': 67,\n",
       " 'E': 68,\n",
       " ']': 69,\n",
       " 'G': 70,\n",
       " 'u': 71,\n",
       " 'z': 72,\n",
       " '`': 73,\n",
       " '-': 74,\n",
       " '2': 75,\n",
       " 'U': 76,\n",
       " 'B': 77,\n",
       " 'f': 78,\n",
       " 'R': 79,\n",
       " '1': 80,\n",
       " ')': 81,\n",
       " 'b': 82,\n",
       " 'L': 83}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "       33, 33, 33, 33, 33, 80, 38, 33, 33, 18, 56, 29, 45, 33, 78, 65, 22,\n",
       "       56, 47,  3, 13, 33,  4, 56, 47, 65, 13, 71, 56, 47,  3, 33, 28, 47,\n",
       "       33, 37, 47,  3, 22, 56, 47, 33, 22, 32,  4, 56, 47, 65,  3, 47, 39,\n",
       "       38, 33, 33,  0, 59, 65, 13, 33, 13, 59, 47, 56, 47, 82, 30, 33, 82,\n",
       "       47, 65, 71, 13, 30, 60,  3, 33, 56, 29,  3, 47, 33, 45, 22, 40, 59,\n",
       "       13, 33, 32, 47, 53, 47, 56, 33, 37, 22, 47, 39, 38, 33, 33, 77, 71,\n",
       "       13, 33, 65,  3, 33, 13, 59, 47, 33, 56, 22, 54, 47, 56, 33,  3, 59,\n",
       "       29, 71, 67, 37, 33, 82, 30, 33, 13, 22, 45, 47, 33, 37, 47,  4, 47,\n",
       "       65,  3, 47, 39, 38, 33, 33, 51, 22,  3, 33, 13, 47, 32, 37, 47, 56,\n",
       "       33, 59, 47, 22, 56, 33, 45, 22, 40, 59, 13, 33, 82, 47, 65, 56, 33,\n",
       "       59, 22,  3, 33, 45, 47, 45, 29, 56, 30, 44, 38, 33, 33, 77, 71, 13,\n",
       "       33, 13, 59, 29, 71, 33,  4, 29, 32, 13, 56, 65,  4, 13, 47, 37, 33,\n",
       "       13, 29, 33, 13, 59, 22, 32, 47, 33, 29, 28, 32, 33, 82, 56, 22, 40,\n",
       "       59, 13, 33, 47, 30, 47,  3, 39, 38, 33, 33, 18, 47, 47, 37, 60,  3,\n",
       "       13, 33, 13, 59, 30, 33, 67, 22, 40, 59, 13, 60,  3, 33, 78, 67, 65,\n",
       "       45, 47, 33, 28, 22, 13, 59, 33,  3, 47, 67, 78, 74,  3, 71, 82,  3,\n",
       "       13, 65, 32, 13, 22, 65, 67, 33, 78, 71, 47, 67, 39, 38, 33, 33, 24,\n",
       "       65, 41, 22, 32, 40, 33, 65, 33, 78, 65, 45, 22, 32, 47, 33, 28, 59,\n",
       "       47, 56, 47, 33, 65, 82, 71, 32, 37, 65, 32,  4, 47, 33, 67, 22, 47,\n",
       "        3, 39, 38, 33, 33,  0, 59, 30, 33,  3, 47, 67, 78, 33, 13, 59, 30,\n",
       "       33, 78, 29, 47, 39, 33, 13, 29, 33, 13, 59, 30, 33,  3, 28, 47, 47,\n",
       "       13, 33,  3, 47, 67, 78, 33, 13, 29, 29, 33,  4, 56, 71, 47, 67, 44,\n",
       "       38, 33, 33,  0, 59, 29, 71, 33, 13, 59, 65, 13, 33, 65, 56, 13, 33,\n",
       "       32, 29, 28, 33, 13, 59, 47, 33, 28, 29, 56, 67, 37, 60,  3, 33, 78,\n",
       "       56, 47,  3, 59, 33, 29, 56, 32, 65, 45, 47, 32, 13, 39, 38, 33, 33,\n",
       "        7, 32, 37, 33, 29, 32, 67, 30, 33, 59, 47, 56, 65, 67, 37, 33, 13,\n",
       "       29, 33, 13, 59, 47, 33, 40, 65, 71, 37, 30, 33,  3, 54, 56, 22, 32,\n",
       "       40, 39, 38, 33, 33, 49, 22, 13, 59, 22, 32, 33, 13, 59, 22, 32, 47,\n",
       "       33, 29, 28, 32, 33, 82, 71])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now all the characters in text is represented by a number\n",
    "\n",
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    # Shape of one_hot will encoded_text_size x num_unique_chars\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    # flatten() turns multi-dim matrix into a 1D array\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "\n",
    "    # Reshape it so it matches the batch shape\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a matrix of shape (3(len of array) x 3(num of unique elements))<br>\n",
    "to represent 1, there is a 1 at index 1<br>\n",
    "to represent 2, there is a 2 at index 2<br>\n",
    "to represent 0, there is a 0 at index 0<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, this function will put one in the array to represent the presence of a word/char in a sentence/word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things before we move on to the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = np.arange(10)\n",
    "example_text\n",
    "# Assuming each number represents a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we wanted to break this into 5 batches\n",
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the batch generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    # This is the same steps we did in the cells above\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 33, 33, ..., 38, 38, 38])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for demostration of what generate_batches() output\n",
    "# Assume sample_text is an encoded represenation of chars\n",
    "# i.e each number represents a char\n",
    "\n",
    "sample_text = np.arange(30)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first batch\n",
    "x, y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This batch of x has the first 10 words broken into 2 samples. So 5 words per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [16, 17, 18, 19, 20]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This batch of y is the same thing as x, but 1 element shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the LSTM Model\n",
    "\n",
    "**Note! We will have options for GPU users and CPU users. CPU will take MUCH LONGER to train and you may encounter RAM issues depending on your hardware. If that is the case, consider using cloud services like AWS, GCP, or Azure. Note, these may cost you money to use!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        \n",
    "        # These are all steps we did in the beginning\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        # len(self.all_chars) is the input size\n",
    "        # num_hidden is the num of features in hidden state\n",
    "        # num_layers is the num of layers\n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # Fully connected linear layer with input_size=num_hidden\n",
    "        # output will be len of all chars, so each output represents a char\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        # Similar to what was done for RNN on time-series\n",
    "        # x is the input\n",
    "        # hidden is the tuple (h_0, c_0)\n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # pass lstm output to a dropout layer\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        # reshape the output so it's ready for the linear layer\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        I removed the GPU part, because it throws error\n",
    "        '''\n",
    "\n",
    "        hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make the total_parameters be roughly the same magnitude as the number of characters in the text.<br>\n",
    "Otherwise, your model may overfit or underfit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model_test.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5447699"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544769"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))\n",
    "train_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cutoff index for our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Feel free to play around with these values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 33, 33, ..., 38, 38, 38])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 60\n",
    "# batch size \n",
    "batch_size = 100\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to examine to output of first iteration to better understand what the model is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (100, 100)\n",
      "x: [38 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 80 38\n",
      " 33 33 18 56 29 45 33 78 65 22 56 47  3 13 33  4 56 47 65 13 71 56 47  3\n",
      " 33 28 47 33 37 47  3 22 56 47 33 22 32  4 56 47 65  3 47 39 38 33 33  0\n",
      " 59 65 13 33 13 59 47 56 47 82 30 33 82 47 65 71 13 30 60  3 33 56 29  3\n",
      " 47 33 45 22]\n",
      "\n",
      "\n",
      "inputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\n",
      "\n",
      "targets: tensor([[33, 33, 33,  ..., 45, 22, 40],\n",
      "        [22, 37, 29,  ..., 22, 47, 39],\n",
      "        [33, 82, 47,  ..., 47, 65, 53],\n",
      "        ...,\n",
      "        [29, 71, 13,  ..., 22, 40, 59],\n",
      "        [39, 33, 33,  ..., 32, 47, 56],\n",
      "        [47, 33, 24,  ...,  7, 30, 39]], dtype=torch.int32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model_test.train()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model_test.hidden_state(batch_size)\n",
    "#     print(\"hidden:\", hidden)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    # batch_size = 100\n",
    "    # seq_len = 100\n",
    "    # So each x will have 100 main arrays, each with 100 elements in it\n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        print(\"x shape:\", x.shape)\n",
    "        print(\"x:\", x[0])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # For tracing and printing purposes\n",
    "        tracker += 1\n",
    "        \n",
    "        \n",
    "        # One Hot Encode incoming data of shape (100,100)\n",
    "        # one_hot_encoder returns x of shape (100,100,84)\n",
    "        # So 100 batch arrays, each with 100 sub-arrays. Each sub-array has 84 elements because\n",
    "        # Remember: there are 84 unique chars in this text data\n",
    "        \n",
    "        # Each number in sub-array represents a word, so each of those words get converted to a 84 length array \n",
    "        # containing 1 where index = encoded_value. \n",
    "        # For example: if 4 represents 'A', there will be a 84 len array with 1 at index 4\n",
    "        \n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        inputs = torch.from_numpy(x)\n",
    "        print('inputs:', inputs)\n",
    "        print(\"\\n\")\n",
    "        targets = torch.from_numpy(y)\n",
    "        print('targets:', targets)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        break\n",
    "    break\n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        # Creating the hidden tuple (h_0, c_0)\n",
    "#         hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "#         model.zero_grad()\n",
    "        \n",
    "#         lstm_output, hidden = model.forward(inputs,hidden)\n",
    "#         loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "#         # LET\"S CLIP JUST IN CASE\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "#         optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "#         if tracker % 25 == 0:\n",
    "            \n",
    "#             val_hidden = model.hidden_state(batch_size)\n",
    "#             val_losses = []\n",
    "#             model.eval()\n",
    "            \n",
    "#             for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "#                 # One Hot Encode incoming data\n",
    "#                 x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "#                 # Convert Numpy Arrays to Tensor\n",
    "\n",
    "#                 inputs = torch.from_numpy(x)\n",
    "#                 targets = torch.from_numpy(y)\n",
    "\n",
    "#                 # Adjust for GPU if necessary\n",
    "\n",
    "#                 if model.use_gpu:\n",
    "\n",
    "#                     inputs = inputs.cuda()\n",
    "#                     targets = targets.cuda()\n",
    "                    \n",
    "#                 # Reset Hidden State\n",
    "#                 # If we dont' reset we would backpropagate through \n",
    "#                 # all training history\n",
    "#                 val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "#                 lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "#                 val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "#                 val_losses.append(val_loss.item())\n",
    "            \n",
    "#             # Reset to training model after val for loop\n",
    "#             model.train()\n",
    "            \n",
    "#             print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden is the tuple (h_0, c_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape  # h_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 is the number of layers\n",
    "- 100 is the batch size\n",
    "- 512 is the num_hidden or hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 512])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[1].shape  # c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 512])\n",
      "torch.Size([3, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "# state.data for state in hidden\n",
    "i=1\n",
    "for state in hidden:\n",
    "    print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 84])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(544769,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 33, 33, ..., 33, 59, 47])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x after one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 84)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This represents 82 which represents '\\n'\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This represents 2 which represents ' '\n",
    "x[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 60\n",
    "# batch size \n",
    "batch_size = 100\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm output: tensor([[ 0.0128,  0.0186, -0.0103,  ..., -0.0350, -0.0245,  0.0400],\n",
      "        [ 0.0169,  0.0246, -0.0188,  ..., -0.0303, -0.0213,  0.0342],\n",
      "        [ 0.0195,  0.0431, -0.0215,  ..., -0.0272, -0.0142,  0.0187],\n",
      "        ...,\n",
      "        [ 0.0348,  0.0319, -0.0147,  ..., -0.0176, -0.0114,  0.0393],\n",
      "        [ 0.0078,  0.0479, -0.0252,  ..., -0.0214, -0.0096,  0.0317],\n",
      "        [ 0.0148,  0.0307, -0.0285,  ..., -0.0338, -0.0167,  0.0373]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "lstm shape: torch.Size([10000, 84])\n",
      "target shape: torch.Size([10000])\n",
      "lstm output: tensor([[ 1.5902e-02,  3.9078e-02, -2.1018e-02,  ..., -3.1244e-02,\n",
      "         -1.6492e-02,  2.9342e-02],\n",
      "        [ 9.3370e-03,  6.1259e-02, -5.4215e-03,  ..., -2.1160e-02,\n",
      "         -2.7503e-02,  2.9383e-02],\n",
      "        [ 9.1621e-03,  2.3921e-02, -1.2162e-02,  ..., -1.8516e-02,\n",
      "         -1.4021e-02,  1.1166e-02],\n",
      "        ...,\n",
      "        [ 2.2576e-02,  3.5748e-02, -1.3561e-03,  ..., -3.3841e-02,\n",
      "         -1.2663e-02,  4.1614e-02],\n",
      "        [ 2.8341e-02,  2.2763e-02, -2.2046e-02,  ..., -2.0739e-02,\n",
      "          5.3674e-03,  3.9772e-02],\n",
      "        [-8.1721e-05,  3.9879e-02, -2.4772e-02,  ..., -4.5266e-02,\n",
      "         -1.4100e-03,  3.6453e-02]], grad_fn=<AddmmBackward>)\n",
      "lstm shape: torch.Size([10000, 84])\n",
      "target shape: torch.Size([10000])\n",
      "lstm output: tensor([[ 0.0207,  0.0457, -0.0212,  ...,  0.0081, -0.0048,  0.0230],\n",
      "        [ 0.0010,  0.0376, -0.0246,  ..., -0.0375, -0.0024,  0.0366],\n",
      "        [ 0.0104,  0.0461, -0.0118,  ..., -0.0349, -0.0204,  0.0155],\n",
      "        ...,\n",
      "        [-0.0034,  0.0370, -0.0095,  ..., -0.0405, -0.0099,  0.0227],\n",
      "        [ 0.0285,  0.0585, -0.0142,  ..., -0.0303, -0.0132,  0.0364],\n",
      "        [ 0.0025,  0.0266, -0.0029,  ..., -0.0165, -0.0188,  0.0124]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "lstm shape: torch.Size([10000, 84])\n",
      "target shape: torch.Size([10000])\n",
      "lstm output: tensor([[ 0.0292,  0.0297, -0.0320,  ..., -0.0232, -0.0046,  0.0384],\n",
      "        [ 0.0204,  0.0244, -0.0125,  ..., -0.0389, -0.0107,  0.0373],\n",
      "        [ 0.0222,  0.0289, -0.0232,  ..., -0.0297, -0.0092,  0.0267],\n",
      "        ...,\n",
      "        [ 0.0148,  0.0454, -0.0186,  ..., -0.0358, -0.0196,  0.0338],\n",
      "        [ 0.0266,  0.0196, -0.0239,  ..., -0.0364, -0.0123,  0.0328],\n",
      "        [ 0.0256,  0.0110, -0.0087,  ..., -0.0377,  0.0061,  0.0226]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "lstm shape: torch.Size([10000, 84])\n",
      "target shape: torch.Size([10000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-d2286bb37a6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# POSSIBLE EXPLODING GRADIENT PROBLEM!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "#         print(\"lstm output:\", lstm_output)\n",
    "#         print(\"lstm shape:\", lstm_output.shape)\n",
    "#         print(\"target shape:\", targets.view(batch_size*seq_len).long().shape)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                \n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets.view(batch_size*seq_len).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.view(batch_size*seq_len).long().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(targets.view(batch_size*seq_len).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
